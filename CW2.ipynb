{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 \n",
    "### Perform k-nearest neighbour (KNN) retrieval experiments according to standard practices in pattern recognition. Use retrieval error (ie @rank1, @rank10) as the performance metric to evaluate different methods. Your baseline approach is KNN on provided features. Use distance metric learning methods to improve a baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, firstly, we need to create a baseline, which is KNN on provided features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "train_idxs = loadmat('cuhk03_new_protocol_config_labeled.mat')['train_idx'].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (14096, 2048)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"PR_data/feature_data.json\", \"r\") as file:\n",
    "    features = json.load(file)\n",
    "    \n",
    "data_features = np.asarray(features)\n",
    "\n",
    "print('Data shape: {}'.format(data_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training indexes : (7368,)\n",
      "query indexes : (1400,)\n",
      "gallery indexes : (5328,)\n"
     ]
    }
   ],
   "source": [
    "#Load labels\n",
    "labelss = loadmat('cuhk03_new_protocol_config_labeled.mat')['labels'].flatten()\n",
    "\n",
    "#Load camId\n",
    "cam_Ids = loadmat('cuhk03_new_protocol_config_labeled.mat')['camId'].flatten()\n",
    "\n",
    "#Load indexes\n",
    "train_idxs = loadmat('cuhk03_new_protocol_config_labeled.mat')['train_idx'].flatten()\n",
    "query_idxs = loadmat('cuhk03_new_protocol_config_labeled.mat')['query_idx'].flatten()\n",
    "gallery_idxs = loadmat('cuhk03_new_protocol_config_labeled.mat')['gallery_idx'].flatten()\n",
    "\n",
    "#Load training indexes\n",
    "print(\"training indexes : {}\".format(train_idxs.shape))\n",
    "print(\"query indexes : {}\".format( query_idxs.shape))\n",
    "print(\"gallery indexes : {}\".format( gallery_idxs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to python index notation\n",
    "\n",
    "train_idxs = train_idxs - 1\n",
    "query_idxs = query_idxs - 1\n",
    "gallery_idxs = gallery_idxs - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the features and data into training, query and gallery sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = []\n",
    "train_label = []\n",
    "train_camid = []\n",
    "# divide the features data into training \n",
    "for i in range (len(train_idxs)):\n",
    "    train_f.append(data_features[train_idxs[i]])\n",
    "    train_camid.append(cam_Ids[train_idxs[i]])\n",
    "    train_label.append(labelss[train_idxs[i]])\n",
    "    \n",
    "train_f = np.asarray(train_f)    \n",
    "train_label = np.asarray(train_label)\n",
    "train_camid = np.asarray(train_camid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features, cam, label data into gallery\n",
    "query_f = []\n",
    "query_camid = []\n",
    "query_label = []\n",
    "\n",
    "for i in range (len(query_idxs)):\n",
    "    query_f.append(data_features[query_idxs[i]])\n",
    "    query_camid.append(cam_Ids[query_idxs[i]])\n",
    "    query_label.append(labelss[query_idxs[i]])\n",
    "    \n",
    "query_f = np.asarray(query_f) \n",
    "query_label = np.asarray(query_label)\n",
    "query_camid = np.asarray(query_camid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features, cam, label data into gallery\n",
    "gallery_f = []\n",
    "gallery_camid = []\n",
    "gallery_label = []\n",
    "\n",
    "for i in range (len(gallery_idxs)):\n",
    "    gallery_f.append(data_features[gallery_idxs[i]])\n",
    "    gallery_camid.append(cam_Ids[gallery_idxs[i]])\n",
    "    gallery_label.append(labelss[gallery_idxs[i]])\n",
    "       \n",
    "gallery_f = np.asarray(gallery_f)  \n",
    "gallery_camid = np.asarray(gallery_camid)  \n",
    "gallery_label = np.asarray(gallery_label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate query and gallery features/labels/cam ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3.    3.    6. ... 1461. 1463. 1463.]\n",
      "(5328, 2050)\n"
     ]
    }
   ],
   "source": [
    "#stacking labels, cam ids and features\n",
    "\n",
    "query = np.vstack((query_f.T, query_label, query_camid)) #transpose feature matrix to match dimensions \n",
    "gallery = np.vstack((gallery_f.T, gallery_label, gallery_camid)) #transpose feature matrix to match dimensions \n",
    "\n",
    "#transpose back to get right shape\n",
    "query = query.T\n",
    "gallery = gallery.T\n",
    "\n",
    "print(query[:,-2])\n",
    "print(gallery.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the repetitions of cam ids and labels and perform NN classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 10)\n",
      "--- 158.0308358669281 seconds ---\n",
      "[[   3]\n",
      " [   3]\n",
      " [   6]\n",
      " ...\n",
      " [1461]\n",
      " [1463]\n",
      " [1463]]\n",
      "(1400, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 10 #number of nearest neighbors parameter\n",
    "\n",
    "rank = []\n",
    "distance = []\n",
    "idx = []\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "##sqeuclidean instead of euclidean for computing efficientcy. As we are only interested in the ranklists\n",
    "##and not the actual values of the distances.\n",
    "\n",
    "for i in range (len(query[:,0])):\n",
    "    \n",
    "    ##getting rid of label/camid repetions \n",
    "    gallery_no_rep = gallery[~np.logical_and((gallery[:,-1] == query[i, -1]), (gallery[:,-2] == query[i, -2]))]\n",
    "    \n",
    "    NN = NearestNeighbors(n_neighbors = k, metric = 'sqeuclidean') #setting up NN\n",
    "    NN.fit(gallery_no_rep[:,:-2], gallery_no_rep[:,-2])\n",
    "\n",
    "   \n",
    "    query_f_test = query_f[i,:].reshape(1,-1)\n",
    "    dist, idxs = NN.kneighbors(query_f_test)\n",
    "    \n",
    "    distance.append(dist)\n",
    "    idx.append(idxs)\n",
    "\n",
    "distance = np.asarray(distance)\n",
    "idx = np.asarray(idx)\n",
    "\n",
    "print(idx.shape)\n",
    "\n",
    "##creating ranklist of predicted labels (1400x10)\n",
    "for i in range (len(query[:,0])):\n",
    "    row_rank = []\n",
    "    for j in range (len(idx[0,:])):\n",
    "        row_rank.append(gallery[idx[i,j],-2])\n",
    "    rank.append(row_rank)\n",
    "    \n",
    "rank = np.asarray(rank)     \n",
    "\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"shape of rank matx: %s\" % (rank.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 10)\n",
      "[[1367.  739. 1426. ... 1449.   51. 1367.]\n",
      " [ 509.  232.  232. ... 1426.  171.    3.]\n",
      " [   6.    6.    6. ...  678.  675.  647.]\n",
      " ...\n",
      " [1418. 1418. 1461. ... 1461. 1418. 1293.]\n",
      " [1463. 1463. 1463. ...  884.  884.  905.]\n",
      " [1463. 1463.  884. ...  884.  346.  905.]]\n"
     ]
    }
   ],
   "source": [
    "rank = np.squeeze(rank)\n",
    "print(rank.shape)\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Match/No Match array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_label = query_label.reshape(1400,1)\n",
    "\n",
    "rank_bin1 = []\n",
    "\n",
    "for i in range (len(rank[:,0])):\n",
    "    rank_bin = []\n",
    "    for j in range (len(rank[0,:])): \n",
    "        if (rank[i,j] == query_label[i]):   ##if match, set 1, otherwise set 0\n",
    "            rank_bin.append(1)\n",
    "        else:\n",
    "            rank_bin.append(0)\n",
    "    rank_bin1.append(rank_bin)\n",
    "    \n",
    "    \n",
    "rank_bin1 = np.asarray(rank_bin1)   ##array of label matches and mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "rank 1: 0.47\n",
      "rank 5: 0.6735714285714286\n",
      "rank 10: 0.7564285714285715\n"
     ]
    }
   ],
   "source": [
    "rank_1 = np.sum(rank_bin1[:,0])/1400\n",
    "r5 = rank_bin1[:,:5].sum(axis = 1)/5\n",
    "rank_5 = np.count_nonzero(r5)/1400\n",
    "r10 = rank_bin1[:,:10].sum(axis = 1)/5\n",
    "rank_10 = np.count_nonzero(r10)/1400\n",
    "\n",
    "print(\"Accuracy\")\n",
    "print(\"rank 1: %s\" % (rank_1))\n",
    "print(\"rank 5: %s\" % (rank_5))\n",
    "print(\"rank 10: %s\" % (rank_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding precisions and recalls for calculating the mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
